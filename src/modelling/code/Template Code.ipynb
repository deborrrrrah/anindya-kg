{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Template Code.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOCjJTa/p62SPFWEnh4qpQO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5VWnbX-sZLU0","colab_type":"text"},"source":["Most of the code is from [Tutorial from Depends on The Definition](https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/) but modified for this final project task and domain."]},{"cell_type":"markdown","metadata":{"id":"Ob4C6KOI5eWQ","colab_type":"text"},"source":["# Setup and Import Library"]},{"cell_type":"code","metadata":{"id":"1hzSN9Wi5QDs","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y43RurGo5a9u","colab_type":"code","colab":{}},"source":["%notebook inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZkSrT0f5b7B","colab_type":"code","colab":{}},"source":["ROOT_PATH = '/content/drive/My Drive/Tugas/Tugas Semester 8/Tugas Akhir/13516152 - Deborah Aprilia Josephine/';"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTJfUcIt5d0a","colab_type":"code","colab":{}},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqiMbRV46ie9","colab_type":"code","colab":{}},"source":["!pip install seqeval"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBDnTl1W6kQ6","colab_type":"code","colab":{}},"source":["import pandas as pd \n","import numpy as np\n","import json\n","from transformers import BertTokenizer, BertConfig, XLMRobertaTokenizer, XLMRobertaConfig, DistilBertTokenizer, DistilBertConfig\n","from transformers import BertForTokenClassification, XLMRobertaForTokenClassification, DistilBertForTokenClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4juf63-F-VA","colab_type":"code","colab":{}},"source":["model_type = \"xlmr\"\n","FINE_TUNING_TYPE = \"full\"\n","VERSION = \"final\"\n","SCENARIO = 3\n","EPOCH_CONST = 25\n","BATCH_SIZE = 8\n","WEIGHT_DECAY_RATE = 0.01\n","TRAINING_SIZE = None\n","TESTING_SIZE = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5dBlVJn9Mgu","colab_type":"code","colab":{}},"source":["experiment_information = model_type + \"_v\" + str(VERSION)\n","if FINE_TUNING_TYPE == \"full\" :\n","  experiment_information += \"_full\"\n","elif FINE_TUNING_TYPE == \"partial\" :\n","  experiment_information += \"_partial\"\n","\n","print (\"Info\", experiment_information)\n","\n","training_filename = \"scenario-\" + str(SCENARIO) + \"-train.json\"\n","testing_filename = \"scenario-\" + str(SCENARIO) + \"-test.json\"\n","\n","MODEL_PATH = ROOT_PATH + \"model/scenario \" + str(SCENARIO) + '/'\n","RESULT_PATH = ROOT_PATH + \"result/scenario \" + str(SCENARIO) + '/'\n","DATASET_PATH = ROOT_PATH + \"dataset/version \" + str(VERSION) + '/'\n","\n","print (MODEL_PATH)\n","print (RESULT_PATH)\n","print (DATASET_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAHKFfYs6lTH","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"KDD6rR5p6nF6","colab_type":"code","colab":{}},"source":["def read_json(filename) :\n","  with open(filename, 'r', encoding=\"utf8\") as f:\n","    obj = json.load(f)\n","  return obj\n","\n","def write_json(obj, filename) :\n","  with open(filename, 'w', encoding=\"utf8\") as outfile:\n","    json.dump(obj, outfile)\n","  print (\"Successfully write JSON obj to\", filename)\n","\n","def write_tsv(obj, filename) :\n","  file = open(filename, \"w\", encoding=\"utf-8\") \n","  for tokens in obj :\n","    for token in tokens :\n","      file.write(token['token'] + \"\\t\" + token['label'] + \"\\n\")\n","    file.write(\"==\\n\\n\")\n","  file.close()\n","  print (\"Successfully write JSON obj to\", filename)\n","\n","def read_tsv(filename) :\n","  file = open(filename, \"r\") \n","  obj = {}\n","  items = []\n","  words = file.read().split(\"==\\n\\n\")\n","  sentences = [[tokens.split(\"\\t\") for tokens in word.split(\"\\n\")] for word in words]\n","  for tokens in sentences :\n","    item = []\n","    for token in tokens :\n","      if len(token) == 2 :\n","        tok_lab = {}\n","        tok_lab['token'] = token[0]\n","        tok_lab['label'] = token[1]\n","        item.append(tok_lab)\n","    if len(item) > 1 :\n","      items.append(item)\n","  obj['tokens_labels'] = items\n","  return obj\n","\n","def write_to_txt(obj, filename, key=None) :\n","  file = open(filename, \"w\", encoding=\"utf-8\") \n","  for items in obj :\n","      texts = []\n","      for item in items :\n","        if key :\n","          texts.append(item[key])\n","        else :\n","          texts.append(item)\n","      file.write(\" \".join(texts) + \"\\n\")\n","  file.close()\n","  print (\"Successfully write JSON obj to\", filename)\n","\n","def read_txt(filename) :\n","  results = open(filename, \"r\").readlines()\n","  return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"reuBNz5QWLE_","colab_type":"code","colab":{}},"source":["LIMIT_SIZE = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1PH0YHF6n-O","colab_type":"code","colab":{}},"source":["training_data = read_tsv(DATASET_PATH + training_filename)\n","print (training_data['tokens_labels'][0][0:LIMIT_SIZE])\n","\n","testing_data = read_tsv(DATASET_PATH + testing_filename)\n","print (testing_data['tokens_labels'][0][0:LIMIT_SIZE])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J01nrXzh6sRB","colab_type":"code","colab":{}},"source":["training_labels = [[object['label'] for object in objects] for objects in training_data[\"tokens_labels\"]]\n","print (training_labels[0][0:LIMIT_SIZE])\n","training_sentences = [[object['token'] for object in objects] for objects in training_data[\"tokens_labels\"]]\n","print (training_sentences[0][0:LIMIT_SIZE])\n","\n","if TRAINING_SIZE :\n","  training_labels = training_labels[:TRAINING_SIZE]\n","  training_sentences = training_sentences[:TRAINING_SIZE]\n","print (\"Jumlah label\", str(len(training_labels)), \" Jumlah kalimat\", str(len(training_sentences)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVhkrr-Y6rum","colab_type":"code","colab":{}},"source":["testing_labels = [[object['label'] for object in objects] for objects in testing_data[\"tokens_labels\"]]\n","print (testing_labels[0][0:LIMIT_SIZE])\n","testing_sentences = [[object['token'] for object in objects] for objects in testing_data[\"tokens_labels\"]]\n","print (testing_sentences[0][0:LIMIT_SIZE])\n","\n","if TESTING_SIZE :\n","  testing_labels = testing_labels[:TESTING_SIZE]\n","  testing_sentences = testing_sentences[:TESTING_SIZE]\n","print (\"Jumlah label\", str(len(testing_labels)), \" Jumlah kalimat\", str(len(testing_sentences)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RXrMO_H6ueO","colab_type":"code","colab":{}},"source":["tag2idx = {\n","    \"1-B-Merek\" : 1,\n","    \"1-I-Merek\" : 2,\n","    \"2-O\" : 3,\n","    \"3-B-NamaProduk\" : 4,\n","    \"3-I-NamaProduk\" : 5,\n","    \"4-B-Varian\" : 6,\n","    \"4-I-Varian\" : 7,\n","    \"5-B-Ukuran\" : 8,\n","    \"5-I-Ukuran\" : 9,\n","    \"6-B-Penggunaan\" : 10,\n","    \"6-I-Penggunaan\" : 11,\n","    \"7-B-Tekstur\" : 12,\n","    \"7-I-Tekstur\" : 13,\n","    \"PAD\" : 14\n","}\n","\n","tag2idx2= {\n","    \"B-Merek\" : 1,\n","    \"I-Merek\" : 2,\n","    \"O\" : 3,\n","    \"B-NamaProduk\" : 4,\n","    \"I-NamaProduk\" : 5,\n","    \"B-Varian\" : 6,\n","    \"I-Varian\" : 7,\n","    \"B-Ukuran\" : 8,\n","    \"I-Ukuran\" : 9,\n","    \"B-Penggunaan\" : 10,\n","    \"I-Penggunaan\" : 11,\n","    \"B-Tekstur\" : 12,\n","    \"I-Tekstur\" : 13,\n","    \"PAD\" : 14\n","}\n","\n","tag_values = [tag for tag in tag2idx2.keys()]\n","print (tag_values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPiCQW9X0g0N","colab_type":"code","colab":{}},"source":["if model_type == \"xlmr\" :\n","  MODEL_NAME = 'xlm-roberta-base'\n","  tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n","  model = XLMRobertaForTokenClassification.from_pretrained(\n","      MODEL_NAME,\n","      num_labels=len(tag2idx),\n","      output_attentions = False,\n","      output_hidden_states = False\n","  )\n","\n","elif model_type == \"mbert\" :\n","  MODEL_NAME = 'bert-base-multilingual-cased'\n","  tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n","  model = BertForTokenClassification.from_pretrained(\n","      MODEL_NAME,\n","      num_labels=len(tag2idx),\n","      output_attentions = False,\n","      output_hidden_states = False\n","  )\n","\n","elif model_type == \"distilbert\" :\n","  MODEL_NAME = 'distilbert-base-multilingual-cased'\n","  tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n","  model = DistilBertForTokenClassification.from_pretrained(\n","      MODEL_NAME,\n","      num_labels=len(tag2idx),\n","      output_attentions = False,\n","      output_hidden_states = False\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVFFJBhE6R67","colab_type":"code","colab":{}},"source":["params = list(model.named_parameters())\n","\n","print('The {:} model has {:} different named parameters.\\n'.format(model_type, len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUdVBOHERrqY","colab_type":"code","colab":{}},"source":["no_decay = ['bias', 'gamma', 'beta']\n","last_layer = ['classifier.weight']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKnsKy0n61jb","colab_type":"text"},"source":["# Training Setup"]},{"cell_type":"code","metadata":{"id":"GayHdcAz6zU7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1597090957178,"user_tz":-420,"elapsed":1193,"user":{"displayName":"Deborah Aprilia Josephine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyhRHl40XtL60mXaDGfeVxG_C2YmtBOdwntGA=s64","userId":"08761981170625811706"}},"outputId":"b9842c38-3d06-440c-a8ce-98604b972a36"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","torch.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.6.0+cu101'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"_wYsGsdN66US","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZQRPDGM68P5","colab_type":"code","colab":{}},"source":["torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8EDMQ8I7KwQ","colab_type":"code","colab":{}},"source":["def tokenize_and_preserve_labels(sentence, text_labels):\n","  tokenized_sentence = []\n","  labels = []\n","\n","  for word, label in zip(sentence, text_labels):\n","\n","    # Tokenize the word and count # of subwords the word is broken into\n","    tokenized_word = tokenizer.tokenize(word)\n","    n_subwords = len(tokenized_word)\n","\n","    # Add the tokenized word to the final tokenized word list\n","    tokenized_sentence.extend(tokenized_word)\n","\n","    # Add the same label to the new list of labels `n_subwords` times\n","    labels.extend([label] * n_subwords)\n","\n","  return tokenized_sentence, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6jx8DID63u6","colab_type":"code","colab":{}},"source":["# MAX_LEN = max(max([len(training_sentence) for training_sentence in training_sentences]), max([len(testing_sentence) for testing_sentence in testing_sentences]))\n","MAX_LEN = max([len(training_sentence) for training_sentence in training_sentences])\n","print (\"Token length\", MAX_LEN, \" Batch size\", BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAm-9nZp7Kyy","colab_type":"code","colab":{}},"source":["training_tokenized_texts_and_labels = [\n","    tokenize_and_preserve_labels(sent, labs)\n","    for sent, labs in zip(training_sentences, training_labels)\n","]\n","\n","training_tokenized_texts = [token_label_pair[0] for token_label_pair in training_tokenized_texts_and_labels]\n","training_labels = [token_label_pair[1] for token_label_pair in training_tokenized_texts_and_labels]\n","\n","training_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in training_tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n","\n","training_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in training_labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n","\n","training_attention_masks = [[float(i != 0.0) for i in ii] for ii in training_input_ids]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvANDY6p4S79","colab_type":"code","colab":{}},"source":["testing_tokenized_texts_and_labels = [\n","    tokenize_and_preserve_labels(sent, labs)\n","    for sent, labs in zip(testing_sentences, testing_labels)\n","]\n","\n","testing_tokenized_texts = [token_label_pair[0] for token_label_pair in testing_tokenized_texts_and_labels]\n","testing_labels = [token_label_pair[1] for token_label_pair in testing_tokenized_texts_and_labels]\n","\n","testing_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in testing_tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n","\n","testing_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in testing_labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n","\n","testing_attention_masks = [[float(i != 0.0) for i in ii] for ii in testing_input_ids]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3U88y1V7K2Z","colab_type":"code","colab":{}},"source":["tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(training_input_ids, training_tags,\n","                                                            random_state=2020, test_size=0.3)\n","tr_masks, val_masks, _, _ = train_test_split(training_attention_masks, training_input_ids,\n","                                             random_state=2020, test_size=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWGPJ5cK7K7N","colab_type":"code","colab":{}},"source":["tr_inputs = torch.tensor(tr_inputs)\n","tr_tags = torch.tensor(tr_tags)\n","tr_masks = torch.tensor(tr_masks)\n","\n","val_inputs = torch.tensor(val_inputs)\n","val_tags = torch.tensor(val_tags)\n","val_masks = torch.tensor(val_masks)\n","\n","test_inputs = torch.tensor(testing_input_ids)\n","test_tags = torch.tensor(testing_tags)\n","test_masks = torch.tensor(testing_attention_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3T96_O4L7K4r","colab_type":"code","colab":{}},"source":["train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","valid_sampler = SequentialSampler(valid_data)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n","\n","test_data = TensorDataset(test_inputs, test_masks, test_tags)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJPoLwnx7omc","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"9LGo8zHV7qJn","colab_type":"code","colab":{}},"source":["import transformers\n","from transformers import AdamW\n","\n","transformers.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjKAGmXx7uTG","colab_type":"code","colab":{}},"source":["model.cuda();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlrmqAqS7uwT","colab_type":"code","colab":{}},"source":["if FINE_TUNING_TYPE == \"full\" :\n","  param_optimizer = list(model.named_parameters())\n","  optimizer_grouped_parameters = [\n","      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': WEIGHT_DECAY_RATE},\n","      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.0}\n","  ]\n","elif FINE_TUNING_TYPE == \"partial\":\n","  param_optimizer = list(model.classifier.named_parameters())\n","  optimizer_grouped_parameters = [\n","      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in last_layer)],\n","       'weight_decay_rate': 0.0},\n","      {'params': [torch.tensor(0) for n, p in param_optimizer if any(nd in n for nd in last_layer)],\n","       'weight_decay_rate': WEIGHT_DECAY_RATE}\n","  ]\n","else :\n","  param_optimizer = list(model.classifier.named_parameters())\n","  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","\n","optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=3e-5,\n","    eps=1e-8\n",")\n","\n","print (FINE_TUNING_TYPE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6aPi81J7v2a","colab_type":"code","colab":{}},"source":["from transformers import get_linear_schedule_with_warmup\n","max_grad_norm = 1.0\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * EPOCH_CONST\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKpFmYjJ7xcI","colab_type":"code","colab":{}},"source":["from seqeval.metrics import f1_score, accuracy_score\n","from seqeval.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kvh1GoH_7xkF","colab_type":"code","colab":{}},"source":["loss_values, validation_loss_values, validation_accuracies, validation_f1scores = [], [], [], []\n","\n","for _ in range(EPOCH_CONST):\n","    model.train()\n","    total_loss = 0\n","\n","    # Training\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        model.zero_grad()\n","        \n","        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","        loss = outputs[0]\n","\n","        loss.backward()\n","        total_loss += loss.item()\n","        \n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(\"Average train loss: {}\".format(avg_train_loss))\n","\n","    loss_values.append(avg_train_loss)\n","\n","    # Validation\n","    model.eval()\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    predictions , true_labels = [], []\n","    for batch in valid_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","        \n","        logits = outputs[1].detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        eval_loss += outputs[0].mean().item()\n","        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","        true_labels.extend(label_ids)\n","\n","    eval_loss = eval_loss / len(valid_dataloader)\n","    validation_loss_values.append(eval_loss)\n","    print(\"Validation loss: {}\".format(eval_loss))\n","\n","    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n","                                for p_i, l_i in zip(p, l) if tag_values[l_i - 1] != \"PAD\"]\n","    valid_tags = [tag_values[l_i] for l in true_labels\n","                                  for l_i in l if tag_values[l_i - 1] != \"PAD\"]\n","    \n","    print(\"Validation Accuracy: {}\".format(accuracy_score(valid_tags, pred_tags)))\n","    validation_accuracies.append(accuracy_score(valid_tags, pred_tags))\n","    \n","    print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))\n","    validation_f1scores.append(f1_score(valid_tags, pred_tags))\n","    \n","    print (classification_report(valid_tags, pred_tags))\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLafMUz4795c","colab_type":"code","colab":{}},"source":["import datetime\n","torch.save(model, MODEL_PATH + experiment_information + \".pth\")\n","# torch.save(model, MODEL_PATH + experiment_information + \"-\" + str(datetime.datetime.now().time()) + \".pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9FzoZIp7xnK","colab_type":"code","colab":{}},"source":["metrics = {}\n","metrics_training = {}\n","metrics_training['loss-values'] = loss_values\n","metrics_training['validation-loss'] = validation_loss_values\n","metrics_training['validation-accuracy'] = validation_accuracies\n","metrics_training['validation-f1'] = validation_f1scores\n","metrics_training['classification_report'] = classification_report(valid_tags, pred_tags)\n","\n","metrics['training'] = metrics_training\n","\n","for key, values in metrics.items() :\n","  print (key, len(values))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-X89pxN72BB","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(loss_values, 'b-o', label=\"training loss\")\n","plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n","\n","plt.title(\"Learning curve\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQBEMKq373m2","colab_type":"code","colab":{}},"source":["model.eval()\n","\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","predictions , true_labels = [], []\n","\n","for batch in test_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","    \n","    logits = outputs[1].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    eval_loss += outputs[0].mean().item()\n","    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","    true_labels.extend(label_ids)\n","\n","eval_loss = eval_loss / len(test_dataloader)\n","validation_loss_values.append(eval_loss)\n","print(\"Validation loss: {}\".format(eval_loss))\n","\n","pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n","                            for p_i, l_i in zip(p, l) if tag_values[l_i - 1] != \"PAD\"]\n","valid_tags = [tag_values[l_i] for l in true_labels\n","                              for l_i in l if tag_values[l_i - 1] != \"PAD\"]\n","\n","print(\"Validation Accuracy: {}\".format(accuracy_score(valid_tags, pred_tags)))\n","validation_accuracies.append(accuracy_score(valid_tags, pred_tags))\n","\n","print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))\n","validation_f1scores.append(f1_score(valid_tags, pred_tags))\n","\n","print (classification_report(valid_tags, pred_tags))\n","print()\n","\n","print (pred_tags[0:LIMIT_SIZE])\n","\n","print (valid_tags[0:LIMIT_SIZE])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1VqLxFDH81D","colab_type":"code","colab":{}},"source":["metrics_testing = {}\n","metrics_testing['loss-values'] = loss_values\n","metrics_testing['validation-loss'] = validation_loss_values\n","metrics_testing['validation-accuracy'] = validation_accuracies\n","metrics_testing['validation-f1'] = validation_f1scores\n","metrics_testing['classification_report'] = classification_report(valid_tags, pred_tags)\n","\n","metrics['testing'] = metrics_testing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHNs20eO75TV","colab_type":"code","colab":{}},"source":["writeJSON(metrics, RESULT_PATH + experiment_information + \".json\")\n","# writeJSON(metrics, RESULT_PATH + experiment_information + \"-\" + str(datetime.datetime.now().time()) + \".json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoeiU5U9Dg3U","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}